{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import activations\n",
    "from statistics import mean\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data into 50% training, 10% testing and 40% reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "DATASET_SIZE = 70000\n",
    "TRAIN_RATIO = 0.5\n",
    "VALIDATION_RATIO = 0.4\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X = np.concatenate([x_train, x_test])\n",
    "y = np.concatenate([y_train, y_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=(1-TRAIN_RATIO))\n",
    "X_ref, X_test, y_ref, y_test = train_test_split(X_val, y_val, test_size=((TEST_RATIO/(VALIDATION_RATIO+TEST_RATIO))))\n",
    "\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
    "\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "X_test = np.reshape(X_test, (-1, 28, 28, 1))\n",
    "\n",
    "X_ref = X_val.astype(\"float32\") / 255.0\n",
    "X_ref = np.reshape(X_val, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the teacher and student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10,activation=activations.sigmoid),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison\n",
    "student1 = keras.models.clone_model(student)\n",
    "student2 = keras.models.clone_model(student)\n",
    "student3 = keras.models.clone_model(student)\n",
    "student4 = keras.models.clone_model(student)\n",
    "student5 = keras.models.clone_model(student)\n",
    "student6 = keras.models.clone_model(student)\n",
    "student7 = keras.models.clone_model(student)\n",
    "student8 = keras.models.clone_model(student)\n",
    "student9 = keras.models.clone_model(student)\n",
    "student10 = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    str(i) = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile student and teacher and train it on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student1.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st1 = student1.predict(X_ref)\n",
    "\n",
    "student2.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st2 = student2.predict(X_ref)\n",
    "\n",
    "student3.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st3 =student3.predict(X_ref)\n",
    "student4.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st4 = student4.predict(X_ref)\n",
    "student5.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st5 =student5.predict(X_ref)\n",
    "student6.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st6 = student6.predict(X_ref)\n",
    "student7.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st7 = student7.predict(X_ref)\n",
    "\n",
    "student8.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st8 = student8.predict(X_ref)\n",
    "\n",
    "student9.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st9 = student9.predict(X_ref)\n",
    "\n",
    "student10.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st10 = student10.predict(X_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teacher predictions is the average of 10 student predcitions\n",
    "s1 = sum(st1)/len(st1)\n",
    "s2 = sum(st2)/len(st2)\n",
    "s3 = sum(st3)/len(st3)\n",
    "s4 = sum(st4)/len(st4)\n",
    "s5 = sum(st5)/len(st5)\n",
    "s6 = sum(st6)/len(st6)\n",
    "s7 = sum(st7)/len(st7)\n",
    "s8 = sum(st8)/len(st8)\n",
    "s9 = sum(st9)/len(st9)\n",
    "s10 = sum(st10)/len(st10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_predictions = (s1+s2+s3+s4+s5+s6+s7+s8+s9+s10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(teacher_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "student_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01, clipnorm=0.5)\n",
    "metrics=tf.keras.metrics.CategoricalAccuracy()\n",
    "teacher_predictions\n",
    "def train_step(x,y,student,s):\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        #teacher_predictions = teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = np.linalg.norm((sum(s)/len(s))-teacher_predictions)\n",
    "            loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "\n",
    "#fit the model here with the training data with only few values to each student\n",
    "#after that distillation loss is the 2 norm distance of teacher and that student\n",
    "\n",
    "#do this for 10 devices,then predict the value, after 10 devices, exchange the values and then again train the model and so on\n",
    "        # Compute gradients\n",
    "        trainable_vars = student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        grads_vars = zip(gradients,trainable_vars)\n",
    "        optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {metrics.name: metrics.result()}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_pred = []\n",
    "train_step(X_train0,y_train0,student1,s1)\n",
    "s1 = student1.predict(X_ref)\n",
    "student_pred.append(sum(s1)/len(s1))\n",
    "train_step(X_train1,y_train1,student2,s2)\n",
    "s2 =student2.predict(X_ref)\n",
    "student_pred.append(sum(s2)/len(s2))\n",
    "train_step(X_train2,y_train2,student3,s3)\n",
    "s3 =student3.predict(X_ref)\n",
    "student_pred.append(sum(s3)/len(s3))\n",
    "train_step(X_train3,y_train3,student4,s4)\n",
    "s4 =student4.predict(X_ref)\n",
    "student_pred.append(sum(s4)/len(s4))\n",
    "train_step(X_train4,y_train4,student5,s5)\n",
    "s5 =student5.predict(X_ref)\n",
    "student_pred.append(sum(s5)/len(s5))\n",
    "train_step(X_train5,y_train5,student6,s6)\n",
    "s6 =student6.predict(X_ref)\n",
    "student_pred.append(sum(s6)/len(s6))\n",
    "train_step(X_train6,y_train6,student7,s7)\n",
    "s7 =student7.predict(X_ref)\n",
    "student_pred.append(sum(s7)/len(s7))\n",
    "train_step(X_train7,y_train7,student8,s8)\n",
    "s8 =student8.predict(X_ref)\n",
    "student_pred.append(sum(s8)/len(s8))\n",
    "train_step(X_train8,y_train8,student9,s9)\n",
    "s9 =student9.predict(X_ref)\n",
    "student_pred.append(sum(s9)/len(s9))\n",
    "train_step(X_train9,y_train9,student10,s10)\n",
    "s10 =student10.predict(X_ref)\n",
    "student_pred.append(sum(s10)/len(s10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(student_pred)):\n",
    "    b = [((sum(student_pred)-student_pred[i]/len(student_pred)-1)-student_pred[i])/len(student_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(s1)/len(s1)+b)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computes the 2 norm distance of teacher and student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = student_scratch.predict(X_val)\n",
    "te = teacher.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum(st)/len(st)\n",
    "b = sum(te)/len(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each devices hold one certain type of data(1st device holds label 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_filter = np.where((y_train==i))\n",
    "    globals()['X_train%s' % i] = X_train[train_filter]\n",
    "    globals()['y_train%s' % i] = y_train[train_filter]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('X_train%s % i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_filter = np.where((y_train == 0))\n",
    "    \n",
    "X_train0 = X_train[train_filter]\n",
    "y_train0  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 1))\n",
    "    \n",
    "X_train1 = X_train[train_filter]\n",
    "y_train1  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 2))\n",
    "    \n",
    "X_train2 = X_train[train_filter]\n",
    "y_train2  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 3))\n",
    "    \n",
    "X_train3 = X_train[train_filter]\n",
    "y_train3  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 4))\n",
    "    \n",
    "X_train4 = X_train[train_filter]\n",
    "y_train4  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 5))\n",
    "    \n",
    "X_train5 = X_train[train_filter]\n",
    "y_train5  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 6))\n",
    "    \n",
    "X_train6 = X_train[train_filter]\n",
    "y_train6  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 7))\n",
    "    \n",
    "X_train7 = X_train[train_filter]\n",
    "y_train7  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 8))\n",
    "    \n",
    "X_train8 = X_train[train_filter]\n",
    "y_train8  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 9))\n",
    "    \n",
    "X_train9 = X_train[train_filter]\n",
    "y_train9  = y_train[train_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the student model on each device and produce the soft decisions on reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predict1 = []\n",
    "history = student_scratch_1.fit(X_train0,y_train0,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)   #teacher prediction\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train1,y_train1,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train2,y_train2,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train3,y_train3,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train4,y_train4,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train5,y_train5,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train6,y_train6,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train7,y_train7,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train8,y_train8,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train9,y_train9,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exchange the soft decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict)):\n",
    "    b = [(sum(new_predict)-new_predict[i]/len(new_predict)-1)-new_predict[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = np.linalg.norm(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "student_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01, clipnorm=0.5)\n",
    "metrics=tf.keras.metrics.CategoricalAccuracy()\n",
    "def train_step(x,y,student):\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = dist1\n",
    "            loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "#fit the model here with the training data with only few values to each student\n",
    "#after that distillation loss is the 2 norm distance of teacher and that student \n",
    "        # Compute gradients\n",
    "        trainable_vars = student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        grads_vars = zip(gradients,trainable_vars)\n",
    "        optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {metrics.name: metrics.result()}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(X_train0,y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide = np.array_split(X_train, 10)\n",
    "divide_label = np.array_split(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(divide)):\n",
    "    x_c = divide[i]\n",
    "    y_c = divide_label[i]\n",
    "    history = train_step(x_c, y_c)\n",
    "    history1 = distiller1.evaluate(X_test,y_test)\n",
    "    distiller1.predict(X_val)\n",
    "    acc = history.history['sparse_categorical_accuracy']\n",
    "    loss = history.history['student_loss']\n",
    "    epochs = range(1,301)\n",
    "    acc_test = history1[0]\n",
    "    loss_test = history1[1]\n",
    "    plt.plot(epochs, acc, 'g', label='acc')\n",
    "    plt.plot(epochs, loss, 'b', label='loss')\n",
    "    plt.title('acc and loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('acc and Loss')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_predict1 = []\n",
    "for i in range(len(divide)):\n",
    "    x_c = divide[i]\n",
    "    y_c = divide_label[i]\n",
    "    train_gen = ImageDataGenerator(featurewise_center=True,\n",
    "                                       width_shift_range=4,\n",
    "                                       height_shift_range=4,\n",
    "                                       horizontal_flip=True)\n",
    "    train_gen.fit(x_c)\n",
    "    student_scratch.fit(train_gen.flow(x_c, y_c, batch_size=64),\n",
    "                                   epochs=3, verbose=0, steps_per_epoch=10)\n",
    "    predict = student_scratch.predict(X_val)\n",
    "    new_predict = sum(predict)/len(predict)\n",
    "    new_predict1.append(new_predict)\n",
    "\n",
    "    #acc = history.history['sparse_categorical_accuracy']\n",
    "    #loss = history.history['student_loss']\n",
    "    #epochs = range(1,11)\n",
    "    #plt.plot(epochs, acc, 'g', label='acc')\n",
    "    #plt.plot(epochs, loss, 'b', label='loss')\n",
    "    #plt.title('acc and loss')\n",
    "    #plt.xlabel('Epochs')\n",
    "    #plt.ylabel('acc and Loss')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict1)):\n",
    "    b = [(sum(new_predict1)-new_predict1[i]/len(new_predict1)-1)-new_predict1[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = np.linalg.norm(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['sparse_categorical_accuracy']\n",
    "loss_val = history.history['student_loss']\n",
    "epochs = range(1,35)\n",
    "plt.plot(epochs, loss_train, 'g', label='Acc')\n",
    "plt.plot(epochs, loss_val, 'b', label='student loss')\n",
    "plt.title('Acc and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = distiller1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "print(new_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict)):\n",
    "    b = [(sum(new_predict)-new_predict[i]/len(new_predict)-1)-new_predict[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
