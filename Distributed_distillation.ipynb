{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import activations\n",
    "from statistics import mean\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data into 50% training, 10% testing and 40% reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "DATASET_SIZE = 70000\n",
    "TRAIN_RATIO = 0.5\n",
    "VALIDATION_RATIO = 0.4\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X = np.concatenate([x_train, x_test])\n",
    "y = np.concatenate([y_train, y_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=(1-TRAIN_RATIO))\n",
    "X_ref, X_test, y_ref, y_test = train_test_split(X_val, y_val, test_size=(0.1))\n",
    "\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
    "\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "X_test = np.reshape(X_test, (-1, 28, 28, 1))\n",
    "\n",
    "X_ref = X_ref.astype(\"float32\") / 255.0\n",
    "X_ref = np.reshape(X_ref, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "3500\n",
      "31500\n",
      "31500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(X_ref))\n",
    "print(len(y_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = np.where((y_train == 0))\n",
    "    \n",
    "X_train0 = X_train[train_filter]\n",
    "y_train0  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 1))\n",
    "    \n",
    "X_train1 = X_train[train_filter]\n",
    "y_train1  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 2))\n",
    "    \n",
    "X_train2 = X_train[train_filter]\n",
    "y_train2  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 3))\n",
    "    \n",
    "X_train3 = X_train[train_filter]\n",
    "y_train3  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 4))\n",
    "    \n",
    "X_train4 = X_train[train_filter]\n",
    "y_train4  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 5))\n",
    "    \n",
    "X_train5 = X_train[train_filter]\n",
    "y_train5  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 6))\n",
    "    \n",
    "X_train6 = X_train[train_filter]\n",
    "y_train6  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 7))\n",
    "    \n",
    "X_train7 = X_train[train_filter]\n",
    "y_train7  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 8))\n",
    "    \n",
    "X_train8 = X_train[train_filter]\n",
    "y_train8  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 9))\n",
    "    \n",
    "X_train9 = X_train[train_filter]\n",
    "y_train9  = y_train[train_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10,activation=activations.sigmoid),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison\n",
    "student1 = keras.models.clone_model(student)\n",
    "student2 = keras.models.clone_model(student)\n",
    "student3 = keras.models.clone_model(student)\n",
    "student4 = keras.models.clone_model(student)\n",
    "student5 = keras.models.clone_model(student)\n",
    "student6 = keras.models.clone_model(student)\n",
    "student7 = keras.models.clone_model(student)\n",
    "student8 = keras.models.clone_model(student)\n",
    "student9 = keras.models.clone_model(student)\n",
    "student10 = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student1.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st1 = student1.predict(X_ref)\n",
    "\n",
    "student2.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st2 = student2.predict(X_ref)\n",
    "\n",
    "student3.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st3 =student3.predict(X_ref)\n",
    "student4.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st4 = student4.predict(X_ref)\n",
    "student5.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st5 =student5.predict(X_ref)\n",
    "student6.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st6 = student6.predict(X_ref)\n",
    "student7.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st7 = student7.predict(X_ref)\n",
    "\n",
    "student8.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st8 = student8.predict(X_ref)\n",
    "\n",
    "student9.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st9 = student9.predict(X_ref)\n",
    "\n",
    "student10.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st10 = student10.predict(X_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teacher predictions is the average of 10 student predcitions\n",
    "s1 = sum(st1)/len(st1)\n",
    "s2 = sum(st2)/len(st2)\n",
    "s3 = sum(st3)/len(st3)\n",
    "s4 = sum(st4)/len(st4)\n",
    "s5 = sum(st5)/len(st5)\n",
    "s6 = sum(st6)/len(st6)\n",
    "s7 = sum(st7)/len(st7)\n",
    "s8 = sum(st8)/len(st8)\n",
    "s9 = sum(st9)/len(st9)\n",
    "s10 = sum(st10)/len(st10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = s1+s2+s3+s4+s5+s6+s7+s8+s9+s10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01, clipnorm=0.5)\n",
    "metrics=tf.keras.metrics.CategoricalAccuracy()\n",
    "student_pred = []\n",
    "def train_step(x,y,student,s,X_ref):\n",
    "        student_pred = []\n",
    "        teacher_predictions = (sum1-s)/9\n",
    "        student.fit(x,y)\n",
    "        # Forward pass of teacher\n",
    "        #teacher_predictions = teacher(x, training=False)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = np.linalg.norm((sum(s)/len(s))-teacher_predictions)\n",
    "            loss = (alpha * student_loss) + (1 - alpha) * distillation_loss\n",
    "\n",
    "#fit the model here with the training data with only few values to each student\n",
    "#after that distillation loss is the 2 norm distance of teacher and that student\n",
    "\n",
    "#do this for 10 devices,then predict the value, after 10 devices, exchange the values and then again train the model and so on\n",
    "        # Compute gradients\n",
    "        trainable_vars = student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        grads_vars = zip(gradients,trainable_vars)\n",
    "        optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {metrics.name: metrics.result()}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        \n",
    "       \n",
    "        a = student.predict(X_ref)\n",
    "        student_pred.append(sum(a)/len(a))\n",
    "        \n",
    "        \n",
    "        ##Test on the X_test data\n",
    "        \n",
    "        # Compute predictions\n",
    "        #y_prediction = student(X, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        #student_loss = student_loss_fn(Y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        #metrics.update_state(Y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        #results1 = {metrics.name: metrics.result()}\n",
    "        #results1.update({\"student_loss\": student_loss})\n",
    "        \n",
    "        #mylist = results1.items()\n",
    "        #x, y = zip(*mylist)\n",
    "        #print(y[1].numpy())\n",
    "        #return y[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 4ms/step - loss: 7.2039e-06 - sparse_categorical_accuracy: 1.0000\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 1.7290e-05 - sparse_categorical_accuracy: 1.0000\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 1.3801e-05 - sparse_categorical_accuracy: 1.0000\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 5.5061e-06 - sparse_categorical_accuracy: 1.0000\n",
      "107/107 [==============================] - 0s 5ms/step - loss: 3.5919e-06 - sparse_categorical_accuracy: 1.0000\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 7.9660e-06 - sparse_categorical_accuracy: 1.0000\n",
      "107/107 [==============================] - 1s 5ms/step - loss: 4.2204e-06 - sparse_categorical_accuracy: 1.0000\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 1.3126e-05 - sparse_categorical_accuracy: 1.0000\n",
      "107/107 [==============================] - 1s 5ms/step - loss: 2.3937e-06 - sparse_categorical_accuracy: 1.0000\n",
      "110/110 [==============================] - 1s 5ms/step - loss: 5.0822e-06 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29340/1710433155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudent_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudent_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstudent_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudent_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstudent_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudent_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mX_train0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mX_train1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mX_train2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_step(X_train0,y_train0,student1,s1,X_ref)\n",
    "    train_step(X_train1,y_train1,student2,s2,X_ref)\n",
    "    train_step(X_train2,y_train2,student3,s3,X_ref)\n",
    "    train_step(X_train3,y_train3,student4,s4,X_ref)\n",
    "    train_step(X_train4,y_train4,student5,s5,X_ref)\n",
    "    train_step(X_train5,y_train5,student6,s6,X_ref)\n",
    "    train_step(X_train6,y_train6,student7,s7,X_ref)\n",
    "    train_step(X_train7,y_train7,student8,s8,X_ref)\n",
    "    train_step(X_train8,y_train8,student9,s9,X_ref)\n",
    "    train_step(X_train9,y_train9,student10,s10,X_ref)\n",
    "    for i in range(len(student_pred)):\n",
    "        b = [((sum(student_pred)-student_pred[i]/len(student_pred)-1)-student_pred[i])/len(student_pred)]\n",
    "    #X_train0 = (X_train0+b)/2\n",
    "    #X_train1 = (X_train1+b)/2\n",
    "    #X_train2 = (X_train2+b)/2\n",
    "    #X_train3 = (X_train3+b)/2\n",
    "    #X_train4 = (X_train4+b)/2\n",
    "    #X_train5 = (X_train5+b)/2\n",
    "    #X_train6 = (X_train6+b)/2\n",
    "    #X_train7 = (X_train7+b)/2\n",
    "    #X_train8 = (X_train8+b)/2\n",
    "    #X_train9 = (X_train9+b)/2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29340/2819528976.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    b = b+i\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "xy = student1.fit(X_train0,y_train0)\n",
    "teacher_predictions = (sum1-s1)/9\n",
    "a = xy.history['loss']\n",
    "b = np.linalg.norm((sum(s1)/len(s1))-teacher_predictions)\n",
    "loss1 = (alpha * a[0]) + ((1 - alpha) * b)\n",
    "print(a)\n",
    "print(b)\n",
    "loss1 = np.float32(loss1)\n",
    "print(loss1)\n",
    "loss1.dtype\n",
    "#print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100):\n",
    "    '''\n",
    "    X    = Matrix of X with added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations,2))\n",
    "    for it in range(iterations):\n",
    "        \n",
    "        prediction = np.dot(X,theta)\n",
    "        \n",
    "        theta = theta -(1/m)*learning_rate*( X.T.dot((prediction - y)))\n",
    "        theta_history[it,:] =theta.T\n",
    "        cost_history[it]  = cal_cost(theta,X,y)\n",
    "        \n",
    "    return theta, cost_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(X_train0,y_train0,student1,s1,X_test,y_test,X_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(student_pred)):\n",
    "    b = [((sum(student_pred)-student_pred[i]/len(student_pred)-1)-student_pred[i])/len(student_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(1,11)\n",
    "y = train_step(X_train0,y_train0,student1,s1,X_test,y_test)\n",
    "plt.plot(a, y,'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "def test_step(x,y,student):\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(student_pred)):\n",
    "    b = [((sum(student_pred)-student_pred[i]/len(student_pred)-1)-student_pred[i])/len(student_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoches = range(1,11)\n",
    "st1_r = []\n",
    "st2_r = []\n",
    "st3_r = []\n",
    "st4_r = []\n",
    "st5_r = []\n",
    "st6_r = []\n",
    "st7_r = []\n",
    "st8_r = []\n",
    "st9_r = []\n",
    "st10_r = []\n",
    "for i in range(1,11):\n",
    "    st1_r.append(train_step(X_train0,y_train0,student1,s1,X_test,y_test))\n",
    "    st2_r.append(train_step(X_train1,y_train1,student2,s2,X_test,y_test))\n",
    "    st3_r.append(train_step(X_train2,y_train2,student3,s3,X_test,y_test))\n",
    "    st4_r.append(train_step(X_train3,y_train3,student4,s4,X_test,y_test))\n",
    "    st5_r.append(train_step(X_train4,y_train4,student5,s5,X_test,y_test))\n",
    "    st6_r.append(train_step(X_train5,y_train5,student6,s6,X_test,y_test))\n",
    "    st7_r.append(train_step(X_train6,y_train6,student7,s7,X_test,y_test))\n",
    "    st8_r.append(train_step(X_train7,y_train7,student8,s8,X_test,y_test))\n",
    "    st9_r.append(train_step(X_train8,y_train8,student9,s9,X_test,y_test))\n",
    "    st10_r.append(train_step(X_train9,y_train9,student10,s10,X_test,y_test))\n",
    "    \n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.plot(epoches,st1_r)\n",
    "plt.plot(epoches,st2_r)\n",
    "plt.plot(epoches,st3_r)\n",
    "plt.plot(epoches,st4_r)\n",
    "plt.plot(epoches,st5_r)\n",
    "plt.plot(epoches,st6_r)\n",
    "plt.plot(epoches,st7_r)\n",
    "plt.plot(epoches,st8_r)\n",
    "plt.plot(epoches,st9_r)\n",
    "plt.plot(epoches,st10_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = range(1,11)\n",
    "st1_r = []\n",
    "st2_r = []\n",
    "st3_r = []\n",
    "st4_r = []\n",
    "st5_r = []\n",
    "st6_r = []\n",
    "st7_r = []\n",
    "st8_r = []\n",
    "st9_r = []\n",
    "st10_r = []\n",
    "for i in range(1,11):\n",
    "    st1_r.append(train_step(X_train0,y_train0,student1,s1,X_test,y_test))\n",
    "    st2_r.append(train_step(X_train1,y_train1,student2,s2,X_test,y_test))\n",
    "    st3_r.append(train_step(X_train2,y_train2,student3,s3,X_test,y_test))\n",
    "    st4_r.append(train_step(X_train3,y_train3,student4,s4,X_test,y_test))\n",
    "    st5_r.append(train_step(X_train4,y_train4,student5,s5,X_test,y_test))\n",
    "    st6_r.append(train_step(X_train5,y_train5,student6,s6,X_test,y_test))\n",
    "    st7_r.append(train_step(X_train6,y_train6,student7,s7,X_test,y_test))\n",
    "    st8_r.append(train_step(X_train7,y_train7,student8,s8,X_test,y_test))\n",
    "    st9_r.append(train_step(X_train8,y_train8,student9,s9,X_test,y_test))\n",
    "    st10_r.append(train_step(X_train9,y_train9,student10,s10,X_test,y_test))\n",
    "    \n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(epoches,st1_r)\n",
    "plt.plot(epoches,st2_r)\n",
    "plt.plot(epoches,st3_r)\n",
    "plt.plot(epoches,st4_r)\n",
    "plt.plot(epoches,st5_r)\n",
    "plt.plot(epoches,st6_r)\n",
    "plt.plot(epoches,st7_r)\n",
    "plt.plot(epoches,st8_r)\n",
    "plt.plot(epoches,st9_r)\n",
    "plt.plot(epoches,st10_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = range(1,11)\n",
    "st1_r = []\n",
    "st2_r = []\n",
    "st3_r = []\n",
    "st4_r = []\n",
    "st5_r = []\n",
    "st6_r = []\n",
    "st7_r = []\n",
    "st8_r = []\n",
    "st9_r = []\n",
    "st10_r = []\n",
    "divide = np.array_split(X_train, 10)\n",
    "divide_label = np.array_split(y_train,10)\n",
    "for i in range(1,11):\n",
    "    st1_r.append(train_step(divide[0],divide_label[0],student1,s1,X_test,y_test))\n",
    "    st2_r.append(train_step(divide[1],divide_label[1],student2,s2,X_test,y_test))\n",
    "    st3_r.append(train_step(divide[2],divide_label[2],student3,s3,X_test,y_test))\n",
    "    st4_r.append(train_step(divide[3],divide_label[3],student4,s4,X_test,y_test))\n",
    "    st5_r.append(train_step(divide[4],divide_label[4],student5,s5,X_test,y_test))\n",
    "    st6_r.append(train_step(divide[5],divide_label[5],student6,s6,X_test,y_test))\n",
    "    st7_r.append(train_step(divide[6],divide_label[6],student7,s7,X_test,y_test))\n",
    "    st8_r.append(train_step(divide[7],divide_label[7],student8,s8,X_test,y_test))\n",
    "    st9_r.append(train_step(divide[8],divide_label[8],student9,s9,X_test,y_test))\n",
    "    st10_r.append(train_step(divide[9],divide_label[9],student10,s10,X_test,y_test))\n",
    "    \n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(epoches,st1_r)\n",
    "plt.plot(epoches,st2_r)\n",
    "plt.plot(epoches,st3_r)\n",
    "plt.plot(epoches,st4_r)\n",
    "plt.plot(epoches,st5_r)\n",
    "plt.plot(epoches,st6_r)\n",
    "plt.plot(epoches,st7_r)\n",
    "plt.plot(epoches,st8_r)\n",
    "plt.plot(epoches,st9_r)\n",
    "plt.plot(epoches,st10_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = range(1,41)\n",
    "st1_r = []\n",
    "st2_r = []\n",
    "st3_r = []\n",
    "st4_r = []\n",
    "st5_r = []\n",
    "st6_r = []\n",
    "st7_r = []\n",
    "st8_r = []\n",
    "st9_r = []\n",
    "st10_r = []\n",
    "divide = np.array_split(X_train, 10)\n",
    "divide_label = np.array_split(y_train,10)\n",
    "for i in range(1,41):\n",
    "    st1_r.append(train_step(divide[0],divide_label[0],student1,s1,X_test,y_test))\n",
    "    st2_r.append(train_step(divide[1],divide_label[1],student2,s2,X_test,y_test))\n",
    "    st3_r.append(train_step(divide[2],divide_label[2],student3,s3,X_test,y_test))\n",
    "    st4_r.append(train_step(divide[3],divide_label[3],student4,s4,X_test,y_test))\n",
    "    st5_r.append(train_step(divide[4],divide_label[4],student5,s5,X_test,y_test))\n",
    "    st6_r.append(train_step(divide[5],divide_label[5],student6,s6,X_test,y_test))\n",
    "    st7_r.append(train_step(divide[6],divide_label[6],student7,s7,X_test,y_test))\n",
    "    st8_r.append(train_step(divide[7],divide_label[7],student8,s8,X_test,y_test))\n",
    "    st9_r.append(train_step(divide[8],divide_label[8],student9,s9,X_test,y_test))\n",
    "    st10_r.append(train_step(divide[9],divide_label[9],student10,s10,X_test,y_test))\n",
    "    \n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(epoches,st1_r)\n",
    "plt.plot(epoches,st2_r)\n",
    "plt.plot(epoches,st3_r)\n",
    "plt.plot(epoches,st4_r)\n",
    "plt.plot(epoches,st5_r)\n",
    "plt.plot(epoches,st6_r)\n",
    "plt.plot(epoches,st7_r)\n",
    "plt.plot(epoches,st8_r)\n",
    "plt.plot(epoches,st9_r)\n",
    "plt.plot(epoches,st10_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student_pred = []\n",
    "train_step(X_train0,y_train0,student1,s1,X_ref,y_ref)\n",
    "s1 = student1.predict(X_ref)\n",
    "student_pred.append(sum(s1)/len(s1))\n",
    "train_step(X_train1,y_train1,student2,s2,X_ref,y_ref)\n",
    "s2 =student2.predict(X_ref)\n",
    "student_pred.append(sum(s2)/len(s2))\n",
    "train_step(X_train2,y_train2,student3,s3,X_ref,y_ref)\n",
    "s3 =student3.predict(X_ref)\n",
    "student_pred.append(sum(s3)/len(s3))\n",
    "train_step(X_train3,y_train3,student4,s4,X_ref,y_ref)\n",
    "s4 =student4.predict(X_ref)\n",
    "student_pred.append(sum(s4)/len(s4))\n",
    "train_step(X_train4,y_train4,student5,s5,X_ref,y_ref)\n",
    "s5 =student5.predict(X_ref)\n",
    "student_pred.append(sum(s5)/len(s5))\n",
    "train_step(X_train5,y_train5,student6,s6,X_ref,y_ref)\n",
    "s6 =student6.predict(X_ref)\n",
    "student_pred.append(sum(s6)/len(s6))\n",
    "train_step(X_train6,y_train6,student7,s7,X_ref,y_ref)\n",
    "s7 =student7.predict(X_ref)\n",
    "student_pred.append(sum(s7)/len(s7))\n",
    "train_step(X_train7,y_train7,student8,s8,X_ref,y_ref)\n",
    "s8 =student8.predict(X_ref)\n",
    "student_pred.append(sum(s8)/len(s8))\n",
    "train_step(X_train8,y_train8,student9,s9,X_ref,y_ref)\n",
    "s9 =student9.predict(X_ref)\n",
    "student_pred.append(sum(s9)/len(s9))\n",
    "train_step(X_train9,y_train9,student10,s10,X_ref,y_ref)\n",
    "s10 =student10.predict(X_ref)\n",
    "student_pred.append(sum(s10)/len(s10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(s10)/len(s10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(student_pred)):\n",
    "    b = [((sum(student_pred)-student_pred[i]/len(student_pred)-1)-student_pred[i])/len(student_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.gradienttape as tape:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computes the 2 norm distance of teacher and student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = student_scratch.predict(X_val)\n",
    "te = teacher.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum(st)/len(st)\n",
    "b = sum(te)/len(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each devices hold one certain type of data(1st device holds label 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_filter = np.where((y_train==i))\n",
    "    globals()['X_train%s' % i] = X_train[train_filter]\n",
    "    globals()['y_train%s' % i] = y_train[train_filter]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('X_train%s % i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the student model on each device and produce the soft decisions on reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predict1 = []\n",
    "history = student_scratch_1.fit(X_train0,y_train0,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)   #teacher prediction\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train1,y_train1,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train2,y_train2,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train3,y_train3,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train4,y_train4,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train5,y_train5,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train6,y_train6,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train7,y_train7,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train8,y_train8,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train9,y_train9,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exchange the soft decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict)):\n",
    "    b = [(sum(new_predict)-new_predict[i]/len(new_predict)-1)-new_predict[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = np.linalg.norm(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "student_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01, clipnorm=0.5)\n",
    "metrics=tf.keras.metrics.CategoricalAccuracy()\n",
    "def train_step(x,y,student):\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = dist1\n",
    "            loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "#fit the model here with the training data with only few values to each student\n",
    "#after that distillation loss is the 2 norm distance of teacher and that student \n",
    "        # Compute gradients\n",
    "        trainable_vars = student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        grads_vars = zip(gradients,trainable_vars)\n",
    "        optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {metrics.name: metrics.result()}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(X_train0,y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide = np.array_split(X_train, 10)\n",
    "divide_label = np.array_split(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(divide)):\n",
    "    x_c = divide[i]\n",
    "    y_c = divide_label[i]\n",
    "    history = train_step(x_c, y_c)\n",
    "    history1 = distiller1.evaluate(X_test,y_test)\n",
    "    distiller1.predict(X_val)\n",
    "    acc = history.history['sparse_categorical_accuracy']\n",
    "    loss = history.history['student_loss']\n",
    "    epochs = range(1,301)\n",
    "    acc_test = history1[0]\n",
    "    loss_test = history1[1]\n",
    "    plt.plot(epochs, acc, 'g', label='acc')\n",
    "    plt.plot(epochs, loss, 'b', label='loss')\n",
    "    plt.title('acc and loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('acc and Loss')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_predict1 = []\n",
    "for i in range(len(divide)):\n",
    "    x_c = divide[i]\n",
    "    y_c = divide_label[i]\n",
    "    train_gen = ImageDataGenerator(featurewise_center=True,\n",
    "                                       width_shift_range=4,\n",
    "                                       height_shift_range=4,\n",
    "                                       horizontal_flip=True)\n",
    "    train_gen.fit(x_c)\n",
    "    student_scratch.fit(train_gen.flow(x_c, y_c, batch_size=64),\n",
    "                                   epochs=3, verbose=0, steps_per_epoch=10)\n",
    "    predict = student_scratch.predict(X_val)\n",
    "    new_predict = sum(predict)/len(predict)\n",
    "    new_predict1.append(new_predict)\n",
    "\n",
    "    #acc = history.history['sparse_categorical_accuracy']\n",
    "    #loss = history.history['student_loss']\n",
    "    #epochs = range(1,11)\n",
    "    #plt.plot(epochs, acc, 'g', label='acc')\n",
    "    #plt.plot(epochs, loss, 'b', label='loss')\n",
    "    #plt.title('acc and loss')\n",
    "    #plt.xlabel('Epochs')\n",
    "    #plt.ylabel('acc and Loss')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict1)):\n",
    "    b = [(sum(new_predict1)-new_predict1[i]/len(new_predict1)-1)-new_predict1[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = np.linalg.norm(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['sparse_categorical_accuracy']\n",
    "loss_val = history.history['student_loss']\n",
    "epochs = range(1,35)\n",
    "plt.plot(epochs, loss_train, 'g', label='Acc')\n",
    "plt.plot(epochs, loss_val, 'b', label='student loss')\n",
    "plt.title('Acc and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = distiller1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "print(new_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict)):\n",
    "    b = [(sum(new_predict)-new_predict[i]/len(new_predict)-1)-new_predict[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
