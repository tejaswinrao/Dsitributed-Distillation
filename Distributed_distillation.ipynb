{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import activations\n",
    "from statistics import mean\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
<<<<<<< HEAD
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import statistics"
=======
    "from keras.preprocessing.image import ImageDataGenerator"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
    "## split the data into 50% training, 10% testing and 40% reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "DATASET_SIZE = 70000\n",
    "TRAIN_RATIO = 0.5\n",
    "VALIDATION_RATIO = 0.4\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X = np.concatenate([x_train, x_test])\n",
    "y = np.concatenate([y_train, y_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 4,
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=(1-TRAIN_RATIO))\n",
    "X_ref, X_test, y_ref, y_test = train_test_split(X_val, y_val, test_size=((TEST_RATIO/(VALIDATION_RATIO+TEST_RATIO))))\n",
    "\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
    "\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "X_test = np.reshape(X_test, (-1, 28, 28, 1))\n",
    "\n",
    "X_ref = X_val.astype(\"float32\") / 255.0\n",
    "X_ref = np.reshape(X_val, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the teacher and student model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10,activation=activations.sigmoid),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison\n",
    "student1 = keras.models.clone_model(student)\n",
    "student2 = keras.models.clone_model(student)\n",
    "student3 = keras.models.clone_model(student)\n",
    "student4 = keras.models.clone_model(student)\n",
    "student5 = keras.models.clone_model(student)\n",
    "student6 = keras.models.clone_model(student)\n",
    "student7 = keras.models.clone_model(student)\n",
    "student8 = keras.models.clone_model(student)\n",
    "student9 = keras.models.clone_model(student)\n",
    "student10 = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 22,
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (Temp/ipykernel_60260/1955666638.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Tejaswi\\AppData\\Local\\Temp/ipykernel_60260/1955666638.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    str(i) = keras.models.clone_model(student)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    str(i) = keras.models.clone_model(student)"
<<<<<<< HEAD
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile student and teacher and train it on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile student and teacher and train it on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student1.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st1 = student1.predict(X_ref)\n",
    "\n",
    "student2.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st2 = student2.predict(X_ref)\n",
    "\n",
    "student3.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st3 =student3.predict(X_ref)\n",
    "student4.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st4 = student4.predict(X_ref)\n",
    "student5.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st5 =student5.predict(X_ref)\n",
    "student6.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st6 = student6.predict(X_ref)\n",
    "student7.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st7 = student7.predict(X_ref)\n",
    "\n",
    "student8.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st8 = student8.predict(X_ref)\n",
    "\n",
    "student9.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st9 = student9.predict(X_ref)\n",
<<<<<<< HEAD
    "\n",
    "student10.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st10 = student10.predict(X_ref)"
=======
    "\n",
    "student10.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "st10 = student10.predict(X_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teacher predictions is the average of 10 student predcitions\n",
    "s1 = sum(st1)/len(st1)\n",
    "s2 = sum(st2)/len(st2)\n",
    "s3 = sum(st3)/len(st3)\n",
    "s4 = sum(st4)/len(st4)\n",
    "s5 = sum(st5)/len(st5)\n",
    "s6 = sum(st6)/len(st6)\n",
    "s7 = sum(st7)/len(st7)\n",
    "s8 = sum(st8)/len(st8)\n",
    "s9 = sum(st9)/len(st9)\n",
    "s10 = sum(st10)/len(st10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_predictions = (s1+s2+s3+s4+s5+s6+s7+s8+s9+s10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52602786 0.40473026 0.49382383 0.39254275 0.6865059  0.46877608\n",
      " 0.47954473 0.5664144  0.579587   0.5330344 ]\n"
     ]
    }
   ],
   "source": [
    "print(teacher_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "student_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01, clipnorm=0.5)\n",
    "metrics=tf.keras.metrics.CategoricalAccuracy()\n",
    "teacher_predictions\n",
    "def train_step(x,y,student,s):\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        #teacher_predictions = teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = np.linalg.norm((sum(s)/len(s))-teacher_predictions)\n",
    "            loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "\n",
    "#fit the model here with the training data with only few values to each student\n",
    "#after that distillation loss is the 2 norm distance of teacher and that student\n",
    "\n",
    "#do this for 10 devices,then predict the value, after 10 devices, exchange the values and then again train the model and so on\n",
    "        # Compute gradients\n",
    "        trainable_vars = student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        grads_vars = zip(gradients,trainable_vars)\n",
    "        optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {metrics.name: metrics.result()}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        print(results)"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teacher predictions is the average of 10 student predcitions\n",
    "s1 = sum(st1)/len(st1)\n",
    "s2 = sum(st2)/len(st2)\n",
    "s3 = sum(st3)/len(st3)\n",
    "s4 = sum(st4)/len(st4)\n",
    "s5 = sum(st5)/len(st5)\n",
    "s6 = sum(st6)/len(st6)\n",
    "s7 = sum(st7)/len(st7)\n",
    "s8 = sum(st8)/len(st8)\n",
    "s9 = sum(st9)/len(st9)\n",
    "s10 = sum(st10)/len(st10)\n"
=======
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.098374724>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.2260168>, 'distillation_loss': 0.48766255}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.1073019>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.338705>, 'distillation_loss': 0.5662153}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.09921154>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.256403>, 'distillation_loss': 0.2603318}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.09256826>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.2865634>, 'distillation_loss': 0.38912323}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.08663249>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.269603>, 'distillation_loss': 0.45038006}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.08390747>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.337907>, 'distillation_loss': 0.28439775}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.11322183>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.3437064>, 'distillation_loss': 0.47737983}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.10663815>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.31654>, 'distillation_loss': 0.6203498}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.102189235>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.3612983>, 'distillation_loss': 0.26090643}\n",
      "{'categorical_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.097171426>, 'student_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.1603668>, 'distillation_loss': 0.38282657}\n"
     ]
    }
   ],
   "source": [
    "student_pred = []\n",
    "train_step(X_train0,y_train0,student1,s1)\n",
    "s1 = student1.predict(X_ref)\n",
    "student_pred.append(sum(s1)/len(s1))\n",
    "train_step(X_train1,y_train1,student2,s2)\n",
    "s2 =student2.predict(X_ref)\n",
    "student_pred.append(sum(s2)/len(s2))\n",
    "train_step(X_train2,y_train2,student3,s3)\n",
    "s3 =student3.predict(X_ref)\n",
    "student_pred.append(sum(s3)/len(s3))\n",
    "train_step(X_train3,y_train3,student4,s4)\n",
    "s4 =student4.predict(X_ref)\n",
    "student_pred.append(sum(s4)/len(s4))\n",
    "train_step(X_train4,y_train4,student5,s5)\n",
    "s5 =student5.predict(X_ref)\n",
    "student_pred.append(sum(s5)/len(s5))\n",
    "train_step(X_train5,y_train5,student6,s6)\n",
    "s6 =student6.predict(X_ref)\n",
    "student_pred.append(sum(s6)/len(s6))\n",
    "train_step(X_train6,y_train6,student7,s7)\n",
    "s7 =student7.predict(X_ref)\n",
    "student_pred.append(sum(s7)/len(s7))\n",
    "train_step(X_train7,y_train7,student8,s8)\n",
    "s8 =student8.predict(X_ref)\n",
    "student_pred.append(sum(s8)/len(s8))\n",
    "train_step(X_train8,y_train8,student9,s9)\n",
    "s9 =student9.predict(X_ref)\n",
    "student_pred.append(sum(s9)/len(s9))\n",
    "train_step(X_train9,y_train9,student10,s10)\n",
    "s10 =student10.predict(X_ref)\n",
    "student_pred.append(sum(s10)/len(s10))"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_predictions = (s1+s2+s3+s4+s5+s6+s7+s8+s9+s10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(teacher_predictions)"
=======
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(student_pred)):\n",
    "    b = [((sum(student_pred)-student_pred[i]/len(student_pred)-1)-student_pred[i])/len(student_pred)]"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "student_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01, clipnorm=0.5)\n",
    "metrics=tf.keras.metrics.CategoricalAccuracy()\n",
    "teacher_predictions\n",
    "def train_step(x,y,student,s):\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        #teacher_predictions = teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = np.linalg.norm((sum(s)/len(s))-teacher_predictions)\n",
    "            loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "\n",
    "#fit the model here with the training data with only few values to each student\n",
    "#after that distillation loss is the 2 norm distance of teacher and that student\n",
    "\n",
    "#do this for 10 devices,then predict the value, after 10 devices, exchange the values and then again train the model and so on\n",
    "        # Compute gradients\n",
    "        trainable_vars = student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        grads_vars = zip(gradients,trainable_vars)\n",
    "        optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {metrics.name: metrics.result()}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        print(results)"
=======
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.4035496 , 0.26853284, 0.39806238, 0.25686318, 0.59206384,\n",
       "        0.26481524, 0.2938503 , 0.46343368, 0.49238664, 0.31730205],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47986075, 0.1686355 , 0.20183606, 0.15762629, 0.63431406,\n",
       "        0.36009577, 0.47434115, 0.5600992 , 0.61917377, 0.1669605 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(s1)/len(s1)+b)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computes the 2 norm distance of teacher and student"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "student_pred = []\n",
    "train_step(X_train0,y_train0,student1,s1)\n",
    "s1 = student1.predict(X_ref)\n",
    "student_pred.append(sum(s1)/len(s1))\n",
    "train_step(X_train1,y_train1,student2,s2)\n",
    "s2 =student2.predict(X_ref)\n",
    "student_pred.append(sum(s2)/len(s2))\n",
    "train_step(X_train2,y_train2,student3,s3)\n",
    "s3 =student3.predict(X_ref)\n",
    "student_pred.append(sum(s3)/len(s3))\n",
    "train_step(X_train3,y_train3,student4,s4)\n",
    "s4 =student4.predict(X_ref)\n",
    "student_pred.append(sum(s4)/len(s4))\n",
    "train_step(X_train4,y_train4,student5,s5)\n",
    "s5 =student5.predict(X_ref)\n",
    "student_pred.append(sum(s5)/len(s5))\n",
    "train_step(X_train5,y_train5,student6,s6)\n",
    "s6 =student6.predict(X_ref)\n",
    "student_pred.append(sum(s6)/len(s6))\n",
    "train_step(X_train6,y_train6,student7,s7)\n",
    "s7 =student7.predict(X_ref)\n",
    "student_pred.append(sum(s7)/len(s7))\n",
    "train_step(X_train7,y_train7,student8,s8)\n",
    "s8 =student8.predict(X_ref)\n",
    "student_pred.append(sum(s8)/len(s8))\n",
    "train_step(X_train8,y_train8,student9,s9)\n",
    "s9 =student9.predict(X_ref)\n",
    "student_pred.append(sum(s9)/len(s9))\n",
    "train_step(X_train9,y_train9,student10,s10)\n",
    "s10 =student10.predict(X_ref)\n",
    "student_pred.append(sum(s10)/len(s10))"
=======
    "st = student_scratch.predict(X_val)\n",
    "te = teacher.predict(X_val)"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(student_pred)):\n",
    "    b = [((sum(student_pred)-student_pred[i]/len(student_pred)-1)-student_pred[i])/len(student_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(s1)/len(s1)+b)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computes the 2 norm distance of teacher and student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = student_scratch.predict(X_val)\n",
    "te = teacher.predict(X_val)"
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum(st)/len(st)\n",
    "b = sum(te)/len(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each devices hold one certain type of data(1st device holds label 1)"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 18,
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_filter = np.where((y_train==i))\n",
    "    globals()['X_train%s' % i] = X_train[train_filter]\n",
    "    globals()['y_train%s' % i] = y_train[train_filter]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each devices hold one certain type of data(1st device holds label 1)"
=======
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_60260/659157653.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('X_train%s % i)\n",
    "    "
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_filter = np.where((y_train==i))\n",
    "    globals()['X_train%s' % i] = X_train[train_filter]\n",
    "    globals()['y_train%s' % i] = y_train[train_filter]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('X_train%s % i)\n",
    "    "
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_filter = np.where((y_train == 0))\n",
    "    \n",
    "X_train0 = X_train[train_filter]\n",
    "y_train0  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 1))\n",
    "    \n",
    "X_train1 = X_train[train_filter]\n",
    "y_train1  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 2))\n",
    "    \n",
    "X_train2 = X_train[train_filter]\n",
    "y_train2  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 3))\n",
    "    \n",
    "X_train3 = X_train[train_filter]\n",
    "y_train3  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 4))\n",
    "    \n",
    "X_train4 = X_train[train_filter]\n",
    "y_train4  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 5))\n",
    "    \n",
    "X_train5 = X_train[train_filter]\n",
    "y_train5  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 6))\n",
    "    \n",
    "X_train6 = X_train[train_filter]\n",
    "y_train6  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 7))\n",
    "    \n",
    "X_train7 = X_train[train_filter]\n",
    "y_train7  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 8))\n",
    "    \n",
    "X_train8 = X_train[train_filter]\n",
    "y_train8  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 9))\n",
    "    \n",
    "X_train9 = X_train[train_filter]\n",
    "y_train9  = y_train[train_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the student model on each device and produce the soft decisions on reference data"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "\n",
    "\n",
    "train_filter = np.where((y_train == 0))\n",
    "    \n",
    "X_train0 = X_train[train_filter]\n",
    "y_train0  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 1))\n",
    "    \n",
    "X_train1 = X_train[train_filter]\n",
    "y_train1  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 2))\n",
    "    \n",
    "X_train2 = X_train[train_filter]\n",
    "y_train2  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 3))\n",
    "    \n",
    "X_train3 = X_train[train_filter]\n",
    "y_train3  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 4))\n",
    "    \n",
    "X_train4 = X_train[train_filter]\n",
    "y_train4  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 5))\n",
    "    \n",
    "X_train5 = X_train[train_filter]\n",
    "y_train5  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 6))\n",
    "    \n",
    "X_train6 = X_train[train_filter]\n",
    "y_train6  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 7))\n",
    "    \n",
    "X_train7 = X_train[train_filter]\n",
    "y_train7  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 8))\n",
    "    \n",
    "X_train8 = X_train[train_filter]\n",
    "y_train8  = y_train[train_filter]\n",
    "\n",
    "train_filter = np.where((y_train == 9))\n",
    "    \n",
    "X_train9 = X_train[train_filter]\n",
    "y_train9  = y_train[train_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the student model on each device and produce the soft decisions on reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predict1 = []\n",
    "history = student_scratch_1.fit(X_train0,y_train0,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)   #teacher prediction\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train1,y_train1,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train2,y_train2,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train3,y_train3,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train4,y_train4,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train5,y_train5,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train6,y_train6,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train7,y_train7,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train8,y_train8,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train9,y_train9,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
    "new_predict1 = []\n",
    "history = student_scratch_1.fit(X_train0,y_train0,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)   #teacher prediction\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train1,y_train1,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train2,y_train2,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train3,y_train3,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train4,y_train4,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train5,y_train5,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train6,y_train6,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train7,y_train7,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "new_predict1.append(new_predict)\n",
    "history =student_scratch_1.fit(X_train8,y_train8,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "history =student_scratch_1.fit(X_train9,y_train9,epochs = 10)\n",
    "predict = student_scratch_1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "new_predict1.append(new_predict)\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,11)\n",
    "plt.plot(epochs, acc, 'g')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
    "## Exchange the soft decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict)):\n",
    "    b = [(sum(new_predict)-new_predict[i]/len(new_predict)-1)-new_predict[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = np.linalg.norm(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "student_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01, clipnorm=0.5)\n",
    "metrics=tf.keras.metrics.CategoricalAccuracy()\n",
    "def train_step(x,y,student):\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = dist1\n",
    "            loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "#fit the model here with the training data with only few values to each student\n",
    "#after that distillation loss is the 2 norm distance of teacher and that student \n",
    "        # Compute gradients\n",
    "        trainable_vars = student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        grads_vars = zip(gradients,trainable_vars)\n",
    "        optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {metrics.name: metrics.result()}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(X_train0,y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 14,
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   "metadata": {},
   "outputs": [],
   "source": [
    "divide = np.array_split(X_train, 10)\n",
    "divide_label = np.array_split(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(divide)):\n",
    "    x_c = divide[i]\n",
    "    y_c = divide_label[i]\n",
    "    history = train_step(x_c, y_c)\n",
    "    history1 = distiller1.evaluate(X_test,y_test)\n",
    "    distiller1.predict(X_val)\n",
    "    acc = history.history['sparse_categorical_accuracy']\n",
    "    loss = history.history['student_loss']\n",
    "    epochs = range(1,301)\n",
    "    acc_test = history1[0]\n",
    "    loss_test = history1[1]\n",
    "    plt.plot(epochs, acc, 'g', label='acc')\n",
    "    plt.plot(epochs, loss, 'b', label='loss')\n",
    "    plt.title('acc and loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('acc and Loss')\n",
    "    plt.legend()\n",
    "plt.show()"
<<<<<<< HEAD
=======
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_predict1 = []\n",
    "for i in range(len(divide)):\n",
    "    x_c = divide[i]\n",
    "    y_c = divide_label[i]\n",
    "    train_gen = ImageDataGenerator(featurewise_center=True,\n",
    "                                       width_shift_range=4,\n",
    "                                       height_shift_range=4,\n",
    "                                       horizontal_flip=True)\n",
    "    train_gen.fit(x_c)\n",
    "    student_scratch.fit(train_gen.flow(x_c, y_c, batch_size=64),\n",
    "                                   epochs=3, verbose=0, steps_per_epoch=10)\n",
    "    predict = student_scratch.predict(X_val)\n",
    "    new_predict = sum(predict)/len(predict)\n",
    "    new_predict1.append(new_predict)\n",
    "\n",
    "    #acc = history.history['sparse_categorical_accuracy']\n",
    "    #loss = history.history['student_loss']\n",
    "    #epochs = range(1,11)\n",
    "    #plt.plot(epochs, acc, 'g', label='acc')\n",
    "    #plt.plot(epochs, loss, 'b', label='loss')\n",
    "    #plt.title('acc and loss')\n",
    "    #plt.xlabel('Epochs')\n",
    "    #plt.ylabel('acc and Loss')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict1)):\n",
    "    b = [(sum(new_predict1)-new_predict1[i]/len(new_predict1)-1)-new_predict1[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = np.linalg.norm(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['sparse_categorical_accuracy']\n",
    "loss_val = history.history['student_loss']\n",
    "epochs = range(1,35)\n",
    "plt.plot(epochs, loss_train, 'g', label='Acc')\n",
    "plt.plot(epochs, loss_val, 'b', label='student loss')\n",
    "plt.title('Acc and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = distiller1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "print(new_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict)):\n",
    "    b = [(sum(new_predict)-new_predict[i]/len(new_predict)-1)-new_predict[i]]\n",
    "    sum1.append(b)"
>>>>>>> 88d032a388d550d97ffcd2aa69953b1384e38474
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_predict1 = []\n",
    "for i in range(len(divide)):\n",
    "    x_c = divide[i]\n",
    "    y_c = divide_label[i]\n",
    "    train_gen = ImageDataGenerator(featurewise_center=True,\n",
    "                                       width_shift_range=4,\n",
    "                                       height_shift_range=4,\n",
    "                                       horizontal_flip=True)\n",
    "    train_gen.fit(x_c)\n",
    "    student_scratch.fit(train_gen.flow(x_c, y_c, batch_size=64),\n",
    "                                   epochs=3, verbose=0, steps_per_epoch=10)\n",
    "    predict = student_scratch.predict(X_val)\n",
    "    new_predict = sum(predict)/len(predict)\n",
    "    new_predict1.append(new_predict)\n",
    "\n",
    "    #acc = history.history['sparse_categorical_accuracy']\n",
    "    #loss = history.history['student_loss']\n",
    "    #epochs = range(1,11)\n",
    "    #plt.plot(epochs, acc, 'g', label='acc')\n",
    "    #plt.plot(epochs, loss, 'b', label='loss')\n",
    "    #plt.title('acc and loss')\n",
    "    #plt.xlabel('Epochs')\n",
    "    #plt.ylabel('acc and Loss')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict1)):\n",
    "    b = [(sum(new_predict1)-new_predict1[i]/len(new_predict1)-1)-new_predict1[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = np.linalg.norm(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['sparse_categorical_accuracy']\n",
    "loss_val = history.history['student_loss']\n",
    "epochs = range(1,35)\n",
    "plt.plot(epochs, loss_train, 'g', label='Acc')\n",
    "plt.plot(epochs, loss_val, 'b', label='student loss')\n",
    "plt.title('Acc and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = distiller1.predict(X_val)\n",
    "new_predict = sum(predict)/len(predict)\n",
    "print(new_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = []\n",
    "for i in range(len(new_predict)):\n",
    "    b = [(sum(new_predict)-new_predict[i]/len(new_predict)-1)-new_predict[i]]\n",
    "    sum1.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
