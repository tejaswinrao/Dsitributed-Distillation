{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64).shuffle(10000)\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.image.central_crop(x, 0.75), y))\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
    "train_dataset = train_dataset.repeat()\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(5000).shuffle(10000)\n",
    "valid_dataset = valid_dataset.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))\n",
    "valid_dataset = valid_dataset.map(lambda x, y: (tf.image.central_crop(x, 0.75), y))\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net_block(input_data, filters, conv_size):\n",
    "  x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.Conv2D(filters, conv_size, activation=None, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.Add()([x, input_data])\n",
    "  x = layers.Activation('relu')(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_res_block(input_data, filters, conv_size):\n",
    "  x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(24, 24, 3))\n",
    "x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "num_res_net_blocks = 0   #0,1,2\n",
    "for i in range(num_res_net_blocks):\n",
    "    x = res_net_block(x, 64, 3)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "res_net_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tejaswi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "195/195 [==============================] - 67s 282ms/step - loss: 2.0075 - acc: 0.2451 - val_loss: 2.4246 - val_acc: 0.1467\n",
      "Epoch 2/30\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 1.7205 - acc: 0.3579 - val_loss: 1.8684 - val_acc: 0.2933\n",
      "Epoch 3/30\n",
      "195/195 [==============================] - 60s 307ms/step - loss: 1.5674 - acc: 0.4317 - val_loss: 1.8887 - val_acc: 0.3829\n",
      "Epoch 4/30\n",
      "195/195 [==============================] - 57s 295ms/step - loss: 1.4418 - acc: 0.4860 - val_loss: 1.9475 - val_acc: 0.3995\n",
      "Epoch 5/30\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 1.3586 - acc: 0.5186 - val_loss: 1.5252 - val_acc: 0.4630\n",
      "Epoch 6/30\n",
      "195/195 [==============================] - 62s 316ms/step - loss: 1.3169 - acc: 0.5396 - val_loss: 1.4403 - val_acc: 0.4877\n",
      "Epoch 7/30\n",
      "195/195 [==============================] - 70s 359ms/step - loss: 1.2492 - acc: 0.5651 - val_loss: 1.3281 - val_acc: 0.5399\n",
      "Epoch 8/30\n",
      "195/195 [==============================] - 59s 303ms/step - loss: 1.2274 - acc: 0.5675 - val_loss: 1.5583 - val_acc: 0.4807\n",
      "Epoch 9/30\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 1.1459 - acc: 0.6002 - val_loss: 1.4149 - val_acc: 0.5323\n",
      "Epoch 10/30\n",
      "195/195 [==============================] - 65s 334ms/step - loss: 1.1445 - acc: 0.6085 - val_loss: 1.3080 - val_acc: 0.5462\n",
      "Epoch 11/30\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 1.0862 - acc: 0.6180 - val_loss: 1.1876 - val_acc: 0.5779\n",
      "Epoch 12/30\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 1.0910 - acc: 0.6238 - val_loss: 1.1448 - val_acc: 0.6118\n",
      "Epoch 13/30\n",
      "195/195 [==============================] - 58s 298ms/step - loss: 1.0163 - acc: 0.6438 - val_loss: 1.3185 - val_acc: 0.5592\n",
      "Epoch 14/30\n",
      "195/195 [==============================] - 57s 293ms/step - loss: 1.0108 - acc: 0.6538 - val_loss: 1.0686 - val_acc: 0.6321\n",
      "Epoch 15/30\n",
      "195/195 [==============================] - 55s 280ms/step - loss: 0.9929 - acc: 0.6611 - val_loss: 1.1410 - val_acc: 0.6066\n",
      "Epoch 16/30\n",
      "195/195 [==============================] - 56s 288ms/step - loss: 0.9888 - acc: 0.6608 - val_loss: 1.2361 - val_acc: 0.5751\n",
      "Epoch 17/30\n",
      "195/195 [==============================] - 59s 302ms/step - loss: 0.9445 - acc: 0.6757 - val_loss: 1.0295 - val_acc: 0.6385\n",
      "Epoch 18/30\n",
      "195/195 [==============================] - 58s 298ms/step - loss: 0.9225 - acc: 0.6816 - val_loss: 1.0682 - val_acc: 0.6328\n",
      "Epoch 19/30\n",
      "195/195 [==============================] - 57s 291ms/step - loss: 0.9215 - acc: 0.6796 - val_loss: 1.1720 - val_acc: 0.6061\n",
      "Epoch 20/30\n",
      "195/195 [==============================] - 56s 289ms/step - loss: 0.8973 - acc: 0.6930 - val_loss: 1.3846 - val_acc: 0.5589\n",
      "Epoch 21/30\n",
      "195/195 [==============================] - 58s 300ms/step - loss: 0.8826 - acc: 0.6970 - val_loss: 1.0173 - val_acc: 0.6530\n",
      "Epoch 22/30\n",
      "195/195 [==============================] - 58s 298ms/step - loss: 0.8704 - acc: 0.7046 - val_loss: 1.0169 - val_acc: 0.6531\n",
      "Epoch 23/30\n",
      "195/195 [==============================] - 57s 294ms/step - loss: 0.8517 - acc: 0.7062 - val_loss: 0.9583 - val_acc: 0.6739\n",
      "Epoch 24/30\n",
      "195/195 [==============================] - 55s 283ms/step - loss: 0.8510 - acc: 0.7138 - val_loss: 1.1169 - val_acc: 0.6151\n",
      "Epoch 25/30\n",
      "195/195 [==============================] - 57s 291ms/step - loss: 0.7900 - acc: 0.7276 - val_loss: 0.9745 - val_acc: 0.6725\n",
      "Epoch 26/30\n",
      "195/195 [==============================] - 58s 296ms/step - loss: 0.8136 - acc: 0.7216 - val_loss: 1.2060 - val_acc: 0.6111\n",
      "Epoch 27/30\n",
      "195/195 [==============================] - 57s 294ms/step - loss: 0.8164 - acc: 0.7227 - val_loss: 0.9264 - val_acc: 0.6823\n",
      "Epoch 28/30\n",
      "195/195 [==============================] - 55s 283ms/step - loss: 0.7942 - acc: 0.7291 - val_loss: 0.9649 - val_acc: 0.6778\n",
      "Epoch 29/30\n",
      "195/195 [==============================] - 58s 299ms/step - loss: 0.7682 - acc: 0.7346 - val_loss: 1.0153 - val_acc: 0.6599\n",
      "Epoch 30/30\n",
      "195/195 [==============================] - 58s 296ms/step - loss: 0.7634 - acc: 0.7381 - val_loss: 0.9097 - val_acc: 0.6903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x265fef33dc0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  keras.callbacks.TensorBoard(log_dir='./log/{}'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")), write_images=True),\n",
    "]\n",
    "res_net_model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "res_net_model.fit(train_dataset, epochs=30, steps_per_epoch=195,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
